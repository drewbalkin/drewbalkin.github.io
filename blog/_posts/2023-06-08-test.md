---
layout: default
title:  "Optimal Portfolio Allocation in Earnings Driven Financial Market"
---

In this model, we will have a stock whose profits (or dividends by our assumption) at time $t$ is given by the process $D_t$ where $D_tdt$ represents the total dividend payment in $[t,t+dt]$ for owning $1$ unit of stock. This dividend process will be given by a GBM with mean $\mu$ and volatility $\sigma_D$. Seperately, there is a market multiple process $M_t$ which represents the amount the market is willing to pay for the stock given its current dividend. Simply, if $S_t$ is stock price at time $t$, then $D_t M_t = S_t$. To model the market multiple process, we model its inverse, the dividend yield process $y_t = \frac{1}{M_t} = \frac{D_t}{S_t}$ with an OU model. This is a common model for interest rates and thus makes sense for earnings yield. Thus our model is:

$$dD_t = \mu D_t dt + \sigma_D D_t dW^1_t$$

$$dy_t = -\theta(y_t-y_{eq})dt + \sigma_y dW^2_t$$

We assume for now that the brownian motions $W^1,W^2$ are independent.

From our exogeneos dividend and dividend yield processes, we have an endogenous stock price process determined by the two. Simply we know $S_t = \frac{D_t}{y_t}$. Now we can model our full problem. Let $X_t$ be the investor's wealth at time and $\pi_t$ be the fraction of wealth in which the investor invests in stocks. 

Over time $[t,t+dt]$ the investor will receive dividends equal to the amount of $\pi_t X_t y_t dt$ as they have $\pi_t X_t$ invested in the stock which has a dividend yield of $y_t$. What is meant by dividend yield here is the instantaneous dividend yield. The second aspect of P&L over $[t,t+dt]$ is given by the change in stock price-- the price return. The price return consists of price change due to dividend change (i.e. dividend growth) and also market multiple changes. Simply however, we can calculate it as follows. The stock price return is $\frac{dS_t}{S_t}$ and $\pi_t X_t$ is invested in the stock so the P&L from stock price change is given by $\pi_t X_t \frac{dS_t}{S_t}$. Adding these two components of P&L together we get that $dX_t = \pi_t X_t y_t dt + \pi_t X_t \frac{dS_t}{S_t}$.

Writing this in terms of our exogenous variables $D_t$ and $y_t$, we get that the dynamics for the wealth process $X_t$ are given by:

$$dX_t = \pi_t X_t y_t dt + \pi_t X_t \frac{d(D_t/y_t)}{D_t/y_t}$$

Applying Ito's lemma, we get that


$$dX_t = \pi_t X_t y_t dt + \pi_t X_t \frac{dD_t}{D_t} + \pi_t X_t \frac{d(1/y_t)}{1/y_t} + \pi_t X_t \frac{dD_t d(1/y_t)}{D_t/y_t} $$

The last term is $0$ as $dW^1_t dW^2_t = 0$. Thus we get 

$$dX_t = \pi_t X_t y_t dt + \pi_t X_t\frac{dD_t}{D_t} + \pi_t X_t \frac{d(1/y_t)}{1/y_t}  $$

Factoring out the total stock allocation $\pi_t X_t$, we see that this is saying that the return of investing in a stock is given by

$$ y_t dt +  \frac{dD_t}{D_t} + \frac{d(1/y_t)}{1/y_t},$$

which is the dividend yield percent + percent dividend growth rate + percent change in multiple. The total return process is the process that models growth of 1 dollar completely invested in stocks with reinvested dividends. By setting $\pi_t = 1$ and thus $X_t = TR_t$ by definition of total return, we get that the total return process is

$$dTR_t =  TR_t[ y_t dt +  \frac{dD_t}{D_t} +  \frac{d(1/y_t)}{1/y_t} ]. $$


Anyway we are concerned with the following problem:


$$dD_t  = \mu D_t dt + \sigma_D D_t dW^1_t $$

$$dy_t  = -\theta(y_t-y_{eq})dt + \sigma_y dW^2_t$$

$$dX_t  = \pi_t X_t y_t dt + \pi_t X_t \frac{dD_t}{D_t} + \pi_t X_t \frac{d(1/y_t)}{1/y_t}$$

$$max_{\pi} E[\log(X_T)]$$


We have two main components of solving this problem. The first is the numerical technique for the parameter estimation based on data, the second is the numerical technique to solve for $\pi$. First we go over the numerical technique to solve for $\pi$.

First we assume that we are looking for closed loop feedback form controls that is $\pi_t = \pi(D_t,y_t,X_t,t)$. In other words, we decide our portfolio allocation based on the current state of our wealth, the dividend, and dividend yield, and the current time. Logically, we should expect to see that in reality $\pi_t = \pi(t,y_t)$. That is the only relevant information to decide the portfolio allocation should be the current time $t$ (because we retire at time T) and the current dividend yield $y_t$ as we decide whether we think stocks are expensive or not, as well as how important that is to the portfolio allocation.

Assuming that these are the only factors that matter, let us look for the optimal control $\pi_t = \pi(t,y_t)$. We approximate the optimal control by looking for the optimal control in a class of Neural networks. This is sufficient since neural networks are universal function approximators so the optimal neural network control can be made close to the true optimal control.

Thus we are solving the following problem. Let $\pi_{NN}(t,y_t;\theta)$ be a neural network with parameters $\theta$ that inputs $(t,y_t)$ and maps it to the fraction portfolio allocation to stocks. Then for this  $\pi_{NN}(\cdot,\cdot;\theta)$ we will get a corresponding distribution of final wealth states $X_T$ for this choice of control. We can then calculate the reward function $E[log(X_T)]$ for this choice of control and then optimize the control by performing gradient ascent on the reward function. 

We will be doing this optimization in PyTorch. First let us create a neural network architecture that takes two inputs $t$ and $y$ and has a single real valued output.


```python
import torch
from torch import nn

class Model(nn.Module):
    #Standard NN
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential( 
            nn.Linear(2,64), 
            nn.Sigmoid(), 
            nn.Linear(64, 64), 
            nn.Sigmoid(),
            nn.Linear(64, 1), 
            nn.Sigmoid() #bounds alloc between 0% and 300% (see final output) as to not short nor get too leveraged.
                           #Mostly to prevent getting 0 wealth during training
            )
    def forward(self,z):
        return 3*self.net(z)
        
       pi_NN = Model()

import numpy as np #still useful to have the numpy library to interact with as well

T = 20 #Choice of final time for ODE.
N_steps = 100 #Number of time discretizations of ODE.
N_sim = 2**10 #number of simulations to do in calculating expected log wealth.
N_epochs = 500
dt = T/N_steps #size of dt for euler scheme

#parameters
mu = .01053 #calibrated from real earnings data
sigma_D = .1295 #calibrated from real earnings data
theta = .126 #calibrated from 1/PE 1860-1990
sigma_y = .01288 #calibrated from 1/PE 1860-1990
y_eq = 0.0783 #calibrated from 1/PE 1860-1990

opt = torch.optim.Adam(pi_NN.parameters(), lr=.0005)


Losses = []

for n in range(N_epochs):
    dW1 = torch.randn(N_sim,N_steps)*np.sqrt(dt)
    dW2 = torch.randn(N_sim,N_steps)*np.sqrt(dt)


    X = np.empty(N_steps+1,'O') #preallocating X. 'O' is so that it stores objects. It will be a numpy array of torch tensors
    y = np.empty(N_steps+1,'O')
    D = np.empty(N_steps+1,'O')
    t = np.empty(N_steps+1,'O')
    pi_val = np.empty(N_steps,'O')

    X[0] = 10*torch.ones(N_sim).unsqueeze(1) #initial condition represents X(0) = 10. Let's just start with $10
    y[0] = (.025*torch.randn(N_sim)+.07).unsqueeze(1).clamp_min(0.005)
    D[0] = torch.ones(N_sim).unsqueeze(1)

    for i in range(0,N_steps):
        t[i] = i*dt*torch.ones(N_sim).unsqueeze(1) #Now, t[i] is the usual time given i steps but stored as a tensor. t is a numpy array of these 1d tensors.

        ty = torch.cat((t[i],y[i]),1) #this is (t,y) but as a torch tesnor. We concatenated to tensors into a new one.
        #ty = torch.cat((t[i],y[i],D[i],X[i]),1)
        
        pi_val[i] = pi_NN(ty) #we get our current portfolio alloc given this y

        #Now we do an ODE step
        D[i+1]  = D[i] + mu*D[i]*dt + sigma_D*D[i]*dW1[:,i].unsqueeze(1)
        y[i+1]  = y[i] -theta*(y[i]-y_eq)*dt + sigma_y*dW2[:,i].unsqueeze(1)
        y[i+1] = y[i+1].clamp_min(0.005) #make 200 multiple as high as we go.... 0.5% div yield.
        X[i+1]  = X[i] + pi_val[i]*X[i]*y[i]*dt + pi_val[i]*X[i]*(D[i+1]  - D[i])/D[i] + pi_val[i]*X[i]*(1/y[i+1]  - 1/y[i])*y[i]
        X[i+1] = X[i+1].clamp_min(0) #ReLU so that if X is negative it gets replaced by 0
        
    Loss = - sum(torch.log(X[-1]+2**(-52)))/N_sim 
    #Loss = - torch.quantile((torch.log(X[-1]+2**(-52))),.1)
    Loss.backward()
    opt.step()
    opt.zero_grad()
    
    Losses.append(Loss)
```
Ok.


